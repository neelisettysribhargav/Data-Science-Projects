{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f35c9b8",
   "metadata": {},
   "source": [
    "NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b837f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\B\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f750eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())   # Prints the available stopword lists in all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794e6249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords in English: \n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "Number of stopwords in English: 198\n",
      "Number of stopwords in English: 198\n"
     ]
    }
   ],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words('english')   # These are the predefined stopwords in English in NLTK\n",
    "print('List of stopwords in English: \\n%s' % nltk_stopwords)\n",
    "print('Number of stopwords in English:',len(nltk_stopwords))   # OR\n",
    "print('Number of stopwords in English: %d' % len(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f7c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Historians currently think that anatomically modern humans have been around for between 200,000 and 300,000 of the planet’s 4.5 billion years.\n",
    "And even though 200,000 years is less than one 20,000th of the history of the planet, it is still a very long time!\n",
    "For context, 200,000 years would represent at least 6,000 generations of your ancestors (your grandparents are only 2 generations from you).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3a74fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\B\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')   # If there is any upadate in the tokenizer function, this will download the latest tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b77761",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = nltk.word_tokenize(text)   # Tokenize the text into words means splitting the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc232e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Historians',\n",
       " 'currently',\n",
       " 'think',\n",
       " 'that',\n",
       " 'anatomically',\n",
       " 'modern',\n",
       " 'humans',\n",
       " 'have',\n",
       " 'been',\n",
       " 'around',\n",
       " 'for',\n",
       " 'between',\n",
       " '200,000',\n",
       " 'and',\n",
       " '300,000',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " '’',\n",
       " 's',\n",
       " '4.5',\n",
       " 'billion',\n",
       " 'years',\n",
       " '.',\n",
       " 'And',\n",
       " 'even',\n",
       " 'though',\n",
       " '200,000',\n",
       " 'years',\n",
       " 'is',\n",
       " 'less',\n",
       " 'than',\n",
       " 'one',\n",
       " '20,000th',\n",
       " 'of',\n",
       " 'the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'still',\n",
       " 'a',\n",
       " 'very',\n",
       " 'long',\n",
       " 'time',\n",
       " '!',\n",
       " 'For',\n",
       " 'context',\n",
       " ',',\n",
       " '200,000',\n",
       " 'years',\n",
       " 'would',\n",
       " 'represent',\n",
       " 'at',\n",
       " 'least',\n",
       " '6,000',\n",
       " 'generations',\n",
       " 'of',\n",
       " 'your',\n",
       " 'ancestors',\n",
       " '(',\n",
       " 'your',\n",
       " 'grandparents',\n",
       " 'are',\n",
       " 'only',\n",
       " '2',\n",
       " 'generations',\n",
       " 'from',\n",
       " 'you',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aeffd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tokens)   # Number of tokens in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f136ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens in NLTK: ['Historians', 'currently', 'think', 'that', 'anatomically', 'modern', 'humans', 'have', 'been', 'around', 'for', 'between', '200,000', 'and', '300,000', 'of', 'the', 'planet', '’', 's', '4.5', 'billion', 'years', '.', 'And', 'even', 'though', '200,000', 'years', 'is', 'less', 'than', 'one', '20,000th', 'of', 'the', 'history', 'of', 'the', 'planet', ',', 'it', 'is', 'still', 'a', 'very', 'long', 'time', '!', 'For', 'context', ',', '200,000', 'years', 'would', 'represent', 'at', 'least', '6,000', 'generations', 'of', 'your', 'ancestors', '(', 'your', 'grandparents', 'are', 'only', '2', 'generations', 'from', 'you', ')', '.']\n",
      "Filtered tokens in NLTK: ['Historians', 'currently', 'think', 'anatomically', 'modern', 'humans', 'around', '200,000', '300,000', 'planet', '’', '4.5', 'billion', 'years', '.', 'even', 'though', '200,000', 'years', 'less', 'one', '20,000th', 'history', 'planet', ',', 'still', 'long', 'time', '!', 'context', ',', '200,000', 'years', 'would', 'represent', 'least', '6,000', 'generations', 'ancestors', '(', 'grandparents', '2', 'generations', ')', '.']\n",
      "Old token count in NLTK: 74\n",
      "New token count in NLTK: 45\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords from the tokens\n",
    "\n",
    "filtered_tokens_nltk = [word for word in nltk_tokens if word.lower() not in nltk_stopwords]\n",
    "print('Original tokens in NLTK:', nltk_tokens)\n",
    "print('Filtered tokens in NLTK:', filtered_tokens_nltk)\n",
    "print('Old token count in NLTK:', len(nltk_tokens))\n",
    "print('New token count in NLTK:', len(filtered_tokens_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbe3e4",
   "metadata": {},
   "source": [
    "SPACY Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fc583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\b\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\b\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\b\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\b\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\b\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\b\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\b\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\b\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\b\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\b\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\b\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\B\\anaconda3\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\B\\anaconda3\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "camelot-py 1.0.0 requires pandas>=2.2.2; python_version >= \"3.10\", but you have pandas 2.1.4 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "libretranslate 1.6.5 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "libretranslate 1.6.5 requires packaging==23.1, but you have packaging 25.0 which is incompatible.\n",
      "libretranslate 1.6.5 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1084c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\B\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"c:\\Users\\B\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Download the spaCy English model if not already installed\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dce2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords in English in spaCy: \n",
      "{'when', 'anyone', 'they', 'sometimes', 'hereupon', 'in', 'by', 'had', 'and', 'was', 'noone', 'nine', 'mostly', 'nor', '’s', 'not', 'throughout', 'already', 'again', 'will', 'top', 'him', 'whereby', 'move', 'only', 'two', 'she', '‘m', 'either', 'mine', 'all', 'third', 'everyone', 'thereafter', 'meanwhile', 'many', 'ever', 'something', 'before', 'their', 'between', 'it', 'almost', 'everywhere', 'using', 'whither', 'bottom', 'however', 'thereupon', 'here', 'through', 'hence', 'that', 'why', 'last', 'elsewhere', 'well', 'amongst', 'towards', 'seemed', 'there', 'ourselves', '’ll', 'under', 'becomes', 'someone', 'hers', \"'s\", 'seems', 'my', 'up', 'one', \"'ve\", 'thence', 'i', 'perhaps', 'another', 'does', 'n’t', 'former', 'along', 'few', 'am', 'since', 'me', 'whenever', 'own', 'regarding', '‘d', 'ours', 'the', 'whoever', 'show', 'formerly', 'how', 'front', 'anywhere', 'upon', 'less', 'among', 'serious', 'us', 'after', 'became', 'hereafter', 'seem', 'name', 'should', 'doing', 'of', 'whereas', 'becoming', 'we', 'others', 'otherwise', 'any', 'same', 'unless', 'with', 'therein', 'other', 'several', 'behind', 'about', '’ve', 'such', 'your', 'over', 'sometime', 'take', 'nevertheless', 'at', 'did', 'once', 'eleven', 're', 'six', '‘s', 'thereby', 'yourselves', 'have', '‘ve', 'although', 'on', 'now', 'whether', 'ten', '’m', 'made', 'because', 'these', 'as', 'whole', 'somewhere', \"'re\", 'per', 'across', 'afterwards', 'some', 'wherever', '’d', 'part', 'yours', 'toward', 'enough', \"'ll\", 'forty', 'may', 'himself', 'could', 'without', 'a', 'five', 'alone', 'each', 'keep', 'therefore', \"n't\", 'often', 'call', 'those', 'even', 'every', 'what', 'full', 'her', 'yet', 'are', 'first', 'until', 'wherein', 'no', 'latter', 'whereafter', 'whence', 'out', 'also', 'fifteen', 'seeming', 'hereby', 'themselves', 'do', 'beside', 'must', 'really', 'go', 'hundred', 'done', 'nobody', 'be', 'below', 'both', 'namely', 'whatever', 'anyway', 'or', 'who', 'would', 'his', 'become', 'an', 'during', 'into', 'been', 'eight', 'least', 'than', 'them', 'ca', 'its', 'much', 'he', 'off', 'whereupon', 'more', 'sixty', 'rather', 'neither', 'never', 'somehow', 'our', 'might', 'myself', 'please', 'give', 'onto', \"'d\", 'then', 'if', 'fifty', '‘re', 'down', 'everything', 'above', 'very', 'this', 'except', 'put', 'used', 'you', 'three', 'were', 'too', 'whose', 'next', 'see', 'can', 'quite', 'while', 'say', 'latterly', 'is', 'due', 'various', 'herself', 'together', 'cannot', 'amount', 'so', '’re', 'nothing', 'thus', 'but', 'where', 'which', 'still', 'make', 'beyond', 'always', '‘ll', 'else', 'get', 'though', 'twelve', 'to', 'most', 'against', 'yourself', 'around', 'herein', 'just', 'thru', 'four', 'further', 'whom', 'itself', 'anyhow', 'side', 'anything', 'within', 'nowhere', 'has', 'empty', 'via', 'indeed', 'besides', 'beforehand', 'n‘t', 'being', 'from', 'for', 'none', 'twenty', 'back', 'moreover', \"'m\"}\n",
      "Number of stopwords in English in spaCy: 326\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS   # Predefined stopwords in English in spaCy\n",
    "print('List of stopwords in English in spaCy: \\n%s' % spacy_stopwords)\n",
    "print('Number of stopwords in English in spaCy: %d' % len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37af6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(text)   # Process the text with spaCy to create a Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a842701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c494fbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Historians',\n",
       " 'currently',\n",
       " 'think',\n",
       " 'that',\n",
       " 'anatomically',\n",
       " 'modern',\n",
       " 'humans',\n",
       " 'have',\n",
       " 'been',\n",
       " 'around',\n",
       " 'for',\n",
       " 'between',\n",
       " '200,000',\n",
       " 'and',\n",
       " '300,000',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " '’s',\n",
       " '4.5',\n",
       " 'billion',\n",
       " 'years',\n",
       " '.',\n",
       " '\\n',\n",
       " 'And',\n",
       " 'even',\n",
       " 'though',\n",
       " '200,000',\n",
       " 'years',\n",
       " 'is',\n",
       " 'less',\n",
       " 'than',\n",
       " 'one',\n",
       " '20,000th',\n",
       " 'of',\n",
       " 'the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'the',\n",
       " 'planet',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'still',\n",
       " 'a',\n",
       " 'very',\n",
       " 'long',\n",
       " 'time',\n",
       " '!',\n",
       " '\\n',\n",
       " 'For',\n",
       " 'context',\n",
       " ',',\n",
       " '200,000',\n",
       " 'years',\n",
       " 'would',\n",
       " 'represent',\n",
       " 'at',\n",
       " 'least',\n",
       " '6,000',\n",
       " 'generations',\n",
       " 'of',\n",
       " 'your',\n",
       " 'ancestors',\n",
       " '(',\n",
       " 'your',\n",
       " 'grandparents',\n",
       " 'are',\n",
       " 'only',\n",
       " '2',\n",
       " 'generations',\n",
       " 'from',\n",
       " 'you',\n",
       " ')',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02084a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_tokens)   # Number of tokens in the text using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37237bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens in spaCy: ['Historians', 'currently', 'think', 'that', 'anatomically', 'modern', 'humans', 'have', 'been', 'around', 'for', 'between', '200,000', 'and', '300,000', 'of', 'the', 'planet', '’s', '4.5', 'billion', 'years', '.', '\\n', 'And', 'even', 'though', '200,000', 'years', 'is', 'less', 'than', 'one', '20,000th', 'of', 'the', 'history', 'of', 'the', 'planet', ',', 'it', 'is', 'still', 'a', 'very', 'long', 'time', '!', '\\n', 'For', 'context', ',', '200,000', 'years', 'would', 'represent', 'at', 'least', '6,000', 'generations', 'of', 'your', 'ancestors', '(', 'your', 'grandparents', 'are', 'only', '2', 'generations', 'from', 'you', ')', '.', '\\n']\n",
      "Filtered tokens in spaCy: ['Historians', 'currently', 'think', 'anatomically', 'modern', 'humans', '200,000', '300,000', 'planet', '4.5', 'billion', 'years', '.', '\\n', '200,000', 'years', '20,000th', 'history', 'planet', ',', 'long', 'time', '!', '\\n', 'context', ',', '200,000', 'years', 'represent', '6,000', 'generations', 'ancestors', '(', 'grandparents', '2', 'generations', ')', '.', '\\n']\n",
      "Old token count in spaCy: 76\n",
      "New token count in spaCy: 39\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_spacy = [word for word in spacy_tokens if word.lower() not in spacy_stopwords]\n",
    "print('Original tokens in spaCy:', spacy_tokens)\n",
    "print('Filtered tokens in spaCy:', filtered_tokens_spacy)\n",
    "print('Old token count in spaCy:', len(spacy_tokens))\n",
    "print('New token count in spaCy:', len(filtered_tokens_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73664bc",
   "metadata": {},
   "source": [
    "GENSIM Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7032c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\B\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\B\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\B\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce185c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'computer',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'didn',\n",
       "           'do',\n",
       "           'does',\n",
       "           'doesn',\n",
       "           'doing',\n",
       "           'don',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'just',\n",
       "           'keep',\n",
       "           'kg',\n",
       "           'km',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'make',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'quite',\n",
       "           'rather',\n",
       "           're',\n",
       "           'really',\n",
       "           'regarding',\n",
       "           'same',\n",
       "           'say',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'unless',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'used',\n",
       "           'using',\n",
       "           'various',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS   # Predefined stopwords in English in Gensim\n",
    "gensim_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74aa49f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_stopwords)   # Number of stopwords in English in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c88514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text in Gensim: Historians currently think anatomically modern humans 200,000 300,000 planet’s 4.5 billion years. And 200,000 years 20,000th history planet, long time! For context, 200,000 years represent 6,000 generations ancestors (your grandparents 2 generations you).\n",
      "Old text length in Gensim: 400\n",
      "New text length in Gensim: 255\n"
     ]
    }
   ],
   "source": [
    "new_text = remove_stopwords(text)   # remove_stopwords method automatically tokenize the text and remove stopwords\n",
    "print('Filtered text in Gensim:', new_text)\n",
    "print('Old text length in Gensim:', len(text))\n",
    "print('New text length in Gensim:', len(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb3dae",
   "metadata": {},
   "source": [
    "scikit-learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d40b95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08cf96d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_STOP_WORDS   # Predefined stopwords in English in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e305b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ENGLISH_STOP_WORDS)   # Number of stopwords in English in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e73fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens in Scikit-learn: ['Historians', 'currently', 'think', 'anatomically', 'modern', 'humans', '200,000', '300,000', 'planet’s', '4.5', 'billion', 'years.', 'And', '200,000', 'years', '20,000th', 'history', 'planet,', 'long', 'time!', 'For', 'context,', '200,000', 'years', 'represent', '6,000', 'generations', 'ancestors', '(your', 'grandparents', '2', 'generations', 'you).']\n",
      "Old token count in Scikit-learn: 400\n",
      "New token count in Scikit-learn: 33\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [word for word in text.split() if word not in ENGLISH_STOP_WORDS]\n",
    "print('Filtered tokens in Scikit-learn:', filtered_tokens)\n",
    "print('Old token count in Scikit-learn:', len(text))\n",
    "print('New token count in Scikit-learn:', len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d2830",
   "metadata": {},
   "source": [
    "Adding/Removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75d30d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing length of stopwords in NLTK: 198\n"
     ]
    }
   ],
   "source": [
    "print('Existing length of stopwords in NLTK:', len(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b12374d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords.append('example1')   # Adding a new stopword to the NLTK list\n",
    "nltk_stopwords.extend(['example2', 'example3'])   # Adding multiple stopwords at once\n",
    "len(nltk_stopwords)   # New length of the NLTK stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13c87fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'example1', 'example2', 'example3']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623eef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords.remove('example1')   # Removing a stopword from the NLTK list\n",
    "nltk_stopwords = [word for word in nltk_stopwords if word not in ['example2', 'example3']]   # Removing multiple stopwords\n",
    "len(nltk_stopwords)   # Length after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a56b2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing length of stopwords in spacy: 326\n"
     ]
    }
   ],
   "source": [
    "print('Existing length of stopwords in spacy:',len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27b9e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords.add('example1')   # Adding a new stopword to the spaCy list\n",
    "spacy_stopwords |= {'example2', 'example3'}   # Adding multiple stopwords at once\n",
    "len(spacy_stopwords)   # New length of the spaCy stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f500b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords.remove('example1')   # Removing a new stopword to the spaCy list\n",
    "spacy_stopwords -= {'example2', 'example3'}   # Removing multiple stopwords at once\n",
    "len(spacy_stopwords)   # Length after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d50019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "gensim_stopwords = STOPWORDS.union(set(['example1', 'example2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_stopwords)   # Number of stopwords in English in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cd35d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_stopwords = STOPWORDS.difference(set(['example1', 'example2']))   # Removing stopwords\n",
    "len(gensim_stopwords)   # Length after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d8a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809729f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc15ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
